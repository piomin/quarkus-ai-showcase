# Select AI provider: openai, mistral, or ollama
ai.model.provider=${AI_MODEL_PROVIDER:openai}

# OpenAI Configuration
quarkus.langchain4j.openai.api-key=${OPENAI_API_KEY:your-openai-api-key}
quarkus.langchain4j.openai.chat-model.model-name=gpt-3.5-turbo
quarkus.langchain4j.openai.chat-model.temperature=0.7

# Mistral AI Configuration
quarkus.langchain4j.mistralai.api-key=${MISTRAL_API_KEY:your-mistral-api-key}
quarkus.langchain4j.mistralai.chat-model.model-name=mistral-small
quarkus.langchain4j.mistralai.chat-model.temperature=0.7

# Ollama Configuration
quarkus.langchain4j.ollama.base-url=${OLLAMA_BASE_URL:http://localhost:11434}
quarkus.langchain4j.ollama.chat-model.model-name=${OLLAMA_MODEL:llama2}
quarkus.langchain4j.ollama.chat-model.temperature=0.7

# HTTP and Logging
quarkus.http.port=8080
quarkus.log.level=INFO

# Development profile overrides
%dev.quarkus.log.console.enable=true
%dev.quarkus.log.console.level=DEBUG